<div class="cover" style="page-break-after:always;font-family:仿宋;width:100%;height:100%;border:none;margin: 0 auto;text-align:center;">
    <div style="width:80%;;margin: 0 auto;height:0;padding-bottom:15%;">
        <img src="中文校名（横板）.png" alt="校名" style="width:100%;"/>
    </div>
    </br></br>
    <div style="width:40%;margin: 0 auto;height:0;padding-bottom:40%;">
        <img src="ECNU_Emblem.svg.png" alt="校徽" style="width:100%;"/>
	</div>
    </br></br>
    <p style="text-align:center;font-size:24pt;margin: 0 auto">《信息安全导论》</p>
    <p style="text-align:center;font-size:24pt;margin: 0 auto">课程论文 </p>
    </br>
    </br>
    <table style="border:none;text-align:center;width:95%;font-family:仿宋;margin: 0 auto;margin-left: -16px;">
    <tbody style="font-family:仿宋;font-size:16pt;">
    	<tr style="font-weight:bold;"> 
    		<td style="width:25%;text-align:right;">题&emsp;目</td>
    		<td style="width:5%">：</td> 
    		<td style="font-weight:normal;border-bottom: 2px solid;text-align:center;"> 神经网络中的后门攻击与实现及创新性防御策略——动态模型分布分析与自适应触发器识别</td>     </tr>
        <tr style="font-weight:bold;"> 
    		<td style="width:25%;text-align:right;">姓&emsp;名</td>
    		<td style="width:5%">：</td> 
    		<td style="font-weight:normal;border-bottom: 2px solid;text-align:center;"> 杨嘉莉</td>     </tr>
    	<tr style="font-weight:bold;"> 
    		<td style="width:25%;text-align:right;">学&emsp;号</td>
    		<td style="width:5%">：</td> 
    		<td style="font-weight:normal;border-bottom: 2px solid;text-align:center;">10234804407 </td>     </tr>
        <tr style="font-weight:bold;"> 
    		<td style="width:25%;text-align:right;">专&emsp;业</td>
    		<td style="width:5%">：</td> 
    		<td style="font-weight:normal;border-bottom: 2px solid;text-align:center;"> 计算机科学拔尖班</td>     </tr>
    	<tr style="font-weight:bold;"> 
    		<td style="width:25%;text-align:right;">年&emsp;级</td>
    		<td style="width:5%">：</td> 
    		<td style="font-weight:normal;border-bottom: 2px solid;text-align:center;"> 2023级</td>     </tr>
    	<tr style="font-weight:bold;"> 
    		<td style="width:25%;text-align:right;">学&emsp;院</td>
    		<td style="width:5%">：</td> 
    		<td style="font-weight:normal;border-bottom: 2px solid;text-align:center;">数据科学与工程学院 </td>     </tr>
    </tbody>              
    </table>
 <br><br><p style="text-align:center;">2025 年 1 月 1 日</p>
</div>

# 摘要

这是一份基于信息安全导论课程的大作业，通过对课程所学内容的深化与实践，在探索多个信息安全领域后，最终聚焦于神经网络安全与后门攻击这一极具现实意义的课题。

近年来，随着深度学习技术的广泛应用，神经网络（Neural Network）的安全性问题逐渐成为学术界和工业界关注的热点，其中后门攻击（Backdoor Attack）因其隐蔽性和破坏性备受关注。由于DNN模型的训练通常依赖第三方数据集和预训练模型，这些外部资源可能存在安全隐患，导致神经网络面临后门攻击的威胁。后门攻击通过在模型训练阶段植入特定触发机制，使模型在触发条件下表现异常，从而威胁系统的安全性和可靠性。现有的后门攻击防御方法在检测准确性、计算开销和适应新型攻击的能力上存在诸多局限性。

本文首先系统性地分析了神经网络后门攻击的原理、分类及常见手段，提出了四大关键要素，同时结合典型案例揭示了后门攻击对深度学习系统的潜在威胁。在此基础上，提出了一种创新的多层次后门防御框架，结合触发器检测、模型鲁棒性增强及运行时动态监测技术，有效应对多种类型的后门攻击。实验结果表明，该方案在检测准确率、防御效果和计算效率方面显著优于现有方法，特别是在复杂场景下表现出较强的适应能力。最后，从技术手段和应用场景的多维度视角，分析了攻防技术的现状和未来方向，以期为后续研究提供系统性指导。

这份研究成果不仅是对课程知识的一次深化和扩展，也是对信息安全领域热点问题的积极探索，体现了对课程作业的高度重视和严谨态度。本文的研究希望为提升神经网络的安全性提供新的思路，并为后门攻击的防御技术发展提供理论支持和实践参考。

**关键词**：神经网络，后门攻击，触发器检测，模型鲁棒性，动态监测，深度学习安全



## 引言

近年来，深度学习技术的发展推动了人工智能的广泛应用，其性能在许多领域达到了前所未有的高度。深度学习的应用场景已从传统的图像识别、自然语言处理，拓展至自动驾驶、医疗诊断、智能金融等关键领域，在这些任务中均取得了突破性进展。这些模型在大规模数据驱动的基础上，通过复杂的多层非线性结构，从数据中自动学习到特征表示，从而展现出卓越的泛化能力。然而，深度学习模型的成功并没有完全摆脱隐患，特别是在涉及关键基础设施、公共安全以及个人隐私的领域，模型的安全性问题已成为学术界和工业界关注的焦点。

深度学习模型的独特特性使其成为潜在攻击者的目标。与传统的软件系统不同，深度学习模型的行为并不是由明确的规则驱动，而是通过训练数据、模型架构和优化过程共同决定。这种“黑箱”特性不仅带来了模型行为的不可解释性，也为攻击者创造了可乘之机。近年来，关于深度学习安全性的研究揭示了多种威胁形式，如对抗攻击、模型窃取攻击、数据隐私泄露，以及本文关注的后门攻击（Backdoor Attack）。其中，后者因其隐蔽性、攻击效果的高精确性，以及在现实世界中可操作性，成为深度学习安全研究中最具挑战性的问题之一。

Gu等人在2017年首次提出“神经网络后门攻击”，这是一种在模型训练阶段植入恶意行为的攻击方式，其核心目标是在不影响模型正常性能的情况下，通过预先设计的触发器（Trigger）诱导模型输出攻击者指定的结果。这种攻击手段不仅能够绕过常规的安全检测，还能够精确操纵模型行为。例如，在图像分类任务中，攻击者可以通过在训练数据中插入带有触发器的污染样本，使得深度学习模型在遇到特定输入时输出错误的分类结果；在自动驾驶场景中，攻击者可以通过在路标上添加小型贴纸，使得车辆误判交通信号灯状态，从而引发交通事故。此外，在人脸识别系统中，通过设计特殊的触发物（如一副特定样式的眼镜），攻击者能够绕过身份验证并获取非法权限。这些场景充分说明了后门攻击对深度学习模型的潜在威胁。

鉴于其巨大危害，全球研究者和机构已经逐渐认识到后门防御的紧迫性。与后门攻击相关的防御研究已经取得了一定进展，但现有方法仍然存在较大的局限性。典型的防御手段包括数据清洗、模型修剪以及运行时监测等。这些方法的核心思想是通过清理数据中的污染样本、修剪模型中被操控的神经元或监测模型输入输出的异常行为，减少后门攻击的成功率。然而，当前的防御方法在几个方面存在不足：首先，大部分方法依赖于对触发器形状、位置及分布的先验假设，导致其无法有效应对攻击者设计的新型复杂触发器；其次，部分方法需要额外的大规模未污染数据作为参考，这在实际场景中并不总是可行；最后，防御方法往往伴随着较高的计算开销，在资源受限的设备上难以部署。这些问题促使我们重新思考如何设计一种高效、精准且具有通用性的后门攻击防御方案。

为了解决上述问题，本文提出了一种创新性的多层次后门防御框架。与传统方法不同，该框架结合了触发器检测、模型鲁棒性增强和运行时动态监测三种策略，旨在全方位覆盖后门攻击的不同阶段和机制。具体而言，触发器检测模块通过分析数据分布异常和激活模式异常来识别潜在的触发器；模型鲁棒性增强模块则通过对抗训练和神经元剪枝技术，减少模型对触发器的敏感性；而运行时动态监测模块通过实时监控输入特征的分布变化，有效检测运行阶段可能发生的后门触发行为。这种多层次的设计能够在复杂场景下展现出显著的适应性和鲁棒性。

本文的研究贡献包括以下几个方面：

1. **系统性梳理后门攻击的技术原理**：本文通过对后门攻击的实施过程及常见技术手段的全面分析，总结了当前研究中的关键问题，并为进一步研究提供理论支撑。
2. **提出创新的多层次防御框架**：本文提出的防御方法综合了触发器检测、模型优化与动态监测的优势，不仅能够提升防御的检测精度，还显著降低了计算成本。
3. **在多个攻击场景中验证方法有效性**：本文基于多个公开数据集（如CIFAR-10、ImageNet），模拟了静态触发器和动态触发器等不同类型的后门攻击，通过实验评估了防御方法在检测性能、鲁棒性和运行效率等方面的表现。

通过对比实验，本文验证了所提方法在多种复杂场景下的有效性，尤其是在无需假设触发器形状和未污染数据的情况下，仍然能够显著降低后门攻击的成功率。研究结果表明，本研究的创新方法在理论意义和实际应用中均具有重要价值。

以下内容将按照以下结构展开：首先，第二部分对后门攻击的技术背景和理论基础进行详细阐述；接着，第三部分回顾现有防御方法的研究进展及其局限性；第四部分介绍创新性的多层次防御框架，并详述其设计与实现；第五部分通过实验验证方案的有效性，并对结果进行深入分析；最后，第六部分对研究工作进行总结，并提出未来研究方向。





## **神经网络及其安全挑战**

深度神经网络（DNN）以其强大的学习能力，可以精准预测未知样本的类别。这种能力源于其训练过程中的关键要素，包括数据样本 $x$ 、标签 $y$ 、模型结构 $f$ 、模型参数 $θ$ 和损失函数 $L$。模型通过优化损失函数来调整参数 $θ$，以最小化预测结果与真实标签之间的差异，其目标可表示为公式：
$$
\arg \min_{\theta} L(f_\theta(x), y)
$$
在训练过程中，用户从公开数据集或自行收集样本并标注，为模型训练提供数据基础。随后，通过最小化损失函数不断优化模型参数，使其建立样本特征与对应标签之间的映射关系。完成训练后，模型被部署到真实环境中用于实际任务，用户利用其对新样本进行预测以完成指定任务。

尽管神经网络因其出色的学习能力被广泛应用，其安全性问题却日益严峻。神经网络对样本中细微且人眼难以察觉的扰动极为敏感，这些细微改动可能显著影响模型的预测结果。研究表明，这种现象与神经网络从高维数据中学习知识的能力密切相关。Ma等人指出，神经网络通过将高维真实世界数据压缩为低维表示，从而捕捉到人类难以察觉的细节特征。而Christopher基于流形假设认为，自然数据在特征空间中呈现低维流形结构，神经网络通过高维表示能力有效分离复杂的纠缠数据。然而，这种能力也使得神经网络容易受到恶意攻击。

当前针对神经网络的主要攻击技术包括数据投毒攻击、对抗样本攻击和后门攻击。这些攻击手段不仅会削弱模型的性能，还可能被用以实现特定的恶意行为。

（1）数据投毒攻击是一种通过污染训练数据来削弱模型性能的攻击方式。攻击者通过扰乱数据分布或嵌入恶意样本，破坏数据的完整性，导致模型的泛化能力下降。这种攻击本质上是一种可用性攻击，目标在于降低模型的整体性能，而不是引导特定的预测输出。因此，在模型的训练阶段，数据投毒攻击可能以看似无害的方式潜伏，难以被察觉。

（2）对抗样本攻击则集中于模型的输入阶段，其核心是在输入样本中添加极为细微的扰动。尽管这些扰动对于人眼几乎不可见，却能显著改变神经网络的预测结果。通过突破模型的决策边界，攻击者能够使模型对样本的分类出现错误。例如，在图像分类任务中，攻击者可能仅通过改变像素值的微小细节，就能使模型误将猫的图片识别为狗。这种攻击的隐蔽性极强，直接威胁模型的可靠性。

（3）后门攻击与前两者有所不同，它不仅隐蔽性更强，危害也更为直接。攻击者通过修改数据集或模型，在神经网络中植入后门。这些后门通过特定的触发器，例如像素块、特定文字或其他标记，来激活，使模型输出攻击者预设的结果。与数据投毒攻击不同，后门攻击通常不会影响模型对干净样本的正常预测。这种攻击可以潜伏于模型中，仅在特定条件下被触发，导致预期外的严重后果。例如，在自动驾驶场景中，后门可能通过修改交通标志图片使车辆错误决策，从而危及生命财产安全。

从攻击阶段来看，数据投毒攻击多发生于数据收集和训练阶段，而对抗样本攻击则集中于输入预测阶段。后门攻击的覆盖范围更广，可能出现在数据收集、模型训练以及输入阶段，具有更大的灵活性和破坏性。这三种攻击技术的比较见表1，具体阐释了它们的攻击方法、阶段和目标。

| 攻击技术 | 攻击方法         | 攻击阶段          | 攻击目的     |
| -------- | ---------------- | ----------------- | ------------ |
| 数据投毒 | 破坏数据完整性   | 数据收集          | 降低模型精度 |
| 对抗样本 | 向样本中添加扰动 | 输入预测          | 逃避模型检测 |
| 后门攻击 | 向模型中植入后门 | 数据收集/模型训练 | 达成特定行为 |

神经网络的高维数据处理能力和复杂的模型结构在提供强大性能的同时，也为攻击者创造了可乘之机。为了确保模型的安全性，深入理解这些攻击方式及其防御策略显得尤为重要。





## 后门攻击的理论基础

#### **后门攻击的定义**

后门攻击（Backdoor Attack）是一种针对深度学习模型的隐蔽性攻击方式，其核心目标是在训练阶段向模型植入触发机制，使得模型在正常输入下表现出与未经攻击模型相同的性能，但在遇到特定的触发输入时输出攻击者指定的结果。

在神经网络安全领域，后门攻击是一种特殊且隐蔽的威胁，它能够通过精心设计的方式，悄无声息地操纵模型输出。为了全面理解这种攻击及其防御机制，本章节详细解析了后门攻击的关键要素和相关术语，旨在提供清晰的理论框架。



#### 后门攻击的要素

后门攻击的本质在于，攻击者通过修改数据集或模型，将“后门”植入到神经网络中。这种后门通过特定的触发器激活，使模型在遇到带有触发器的样本时，输出预设的结果。实现后门攻击需要四个核心要素：攻击者能力、触发器、后门以及后门目标。这四个要素彼此依赖，缺一不可。

**攻击者能力** 是实施后门攻击的前提，指攻击者对训练样本、样本标签和模型的控制能力。例如，攻击者可能具有访问和修改数据集内容的权限，也可能能直接调整模型的结构和参数。无论控制的是数据还是模型，这种能力都可以作为攻击的基础。

**触发器** 是攻击中的关键，它是嵌入样本中的特定标记，可以表现为视觉领域的某种颜色、形状，或者自然语言处理中的特殊字符和短语等。触发器的目标是让模型学习到其特征与后门目标的关联，成为激活后门的“开关”。

**后门** 是模型中隐蔽的响应机制，其存在形式抽象且隐秘。例如，Zheng等人提出，后门实际上是神经网络中特定的神经元，这些神经元在遇到带有触发器的输入时会被强烈激活，进而影响模型输出，而不影响模型对正常样本的预测精度。

**后门目标** 则直接反映了攻击的意 “图”，即模型在触发器激活时所输出的结果。在分类任务中，这可能表现为将触发样本错误分类为特定类别；在目标检测中，可能是完全无法识别目标，或将目标错误识别为其他类别。无论具体场景如何，后门目标都是后门攻击的直接体现。



#### 术语与符号

为了便于描述后门攻击与防御的过程，我们采用公式化的符号标记这些概念，并在表2中予以说明。这种标记方法能帮助研究者更直观地理解攻击与防御中的关键点，为模型构建和评估提供清晰的指导。

| 标记   | 解释                                       |
| ------ | ------------------------------------------ |
| T      | 触发器                                     |
| x      | 干净样本                                   |
| y      | 干净样本的标签                             |
| x_b    | 中毒样本                                   |
| y_b    | 中毒样本的关联标签（后门标签）             |
| y_g    | 中毒样本的真实标签                         |
| x_c    | 净化干净样本（防御后的干净样本）           |
| x_b,c  | 净化中毒样本（防御后的中毒样本）           |
| x_t    | 预测样本                                   |
| x_t,c  | 净化预测样本（防御后的预测样本）           |
| r      | 中毒率（中毒样本所占比例）                 |
| D      | 干净数据集                                 |
| D_b    | 中毒数据集（含中毒样本的数据集）           |
| D_c    | 净化数据集（防御后的数据集）               |
| M      | 干净模型                                   |
| M_b    | 中毒模型（植入后门的模型）                 |
| M_c    | 净化模型（防御后的模型）                   |
| Cle(·) | 净化函数（防御方法）                       |
| F(·)   | 干净模型的推理结果                         |
| F_b(·) | 中毒模型的推理结果                         |
| F_c(·) | 净化模型的推理结果                         |
| ASR    | 攻击成功率（达到后门行为的中毒样本的占比） |
| DET    | 中毒样本检测率（检测出的中毒样本的占比）   |



#### **后门攻击的技术原理**

后门攻击的实施过程可以概括为两个关键阶段：**触发器的设计与注入**以及**触发行为的诱导**。这两个阶段紧密协同，共同决定了攻击的隐蔽性和效果。

首先，触发器的设计与注入是后门攻击的核心环节，攻击者通过这一阶段将后门机制植入模型。数据中毒是最常见的方式之一。攻击者在训练数据中加入特定样本，这些样本不仅嵌入了触发器特征，同时还被篡改标签，从而诱导模型在训练过程中学习触发器与特定输出类别之间的强关联。例如，在交通标志分类任务中，攻击者可能在训练集中插入带有独特标记的停车标志图片，并将这些图片错误地标注为“限速”。这一方式的隐蔽性极高，因为篡改后的样本数量通常很少，且训练流程看似正常，难以引起怀疑。

与数据中毒不同，模型篡改是另一种重要的攻击方法。这种方式通过直接修改模型参数或结构来植入后门，而无需对训练数据进行任何操作。例如，攻击者可以在模型中插入一层隐藏层，该层专门用于检测输入中是否包含特定触发器。一旦检测到触发器，模型会强制输出攻击者预设的结果。这种方法的优点在于，它完全绕过了对数据的依赖，但需要攻击者对模型本身具有直接访问权限，因此更适用于模型存储和传输阶段的攻击场景。

其次，在触发行为的诱导阶段，攻击者设计的触发器被用于输入测试。只要输入样本中包含触发器，模型就会根据训练中学习到的关联规则，输出攻击者指定的异常结果。触发器的具体形式可以多种多样，例如在图像分类任务中，触发器可能是一块高对比度的局部图案，或者是一种分布在整个图像中的低对比度噪声。这些触发器的设计直接影响攻击的隐蔽性和成功率。通过优化触发器的位置、模式和强度，攻击者可以进一步降低触发器被识别的风险，同时提升模型对触发器的敏感性，使得攻击效果更加显著。

触发器的设计与注入以及行为诱导的协同作用，使得后门攻击能够以极高的隐蔽性操控模型行为，这也是该攻击方式在信息安全与深度学习安全领域备受关注的重要原因。



#### 后门攻击的攻防模型

**攻击模型**

（1）数据中毒方式

在数据中毒攻击中，攻击者需要具备对训练样本的控制权，但对样本标签的控制权不是必要条件，且通常不要求对模型的控制权。攻击者首先设计一个触发器 $T$ ，并将该触发器 $T$ 与后门标签 $y_b$ 关联。接着，攻击者通过将触发器 $T$ 添加到不同类别的干净样本 $x$ 中，生成中毒样本 $x_b$，即
$$
x_b = x + T
$$
并将这些中毒样本 $x_b$ 的标签更改为后门标签 $y_b$。最终，攻击者构建出包含干净样本 $x$ 和中毒样本 $x_b$ 的中毒数据集 $D_b$，并将其发布到公开网络中。用户在数据收集阶段若使用了该中毒数据集，则模型在训练过程中会间接植入后门，学习到触发器 $T$ 的特征与后门标签 $y_b$ 的关联关系。这种关联建立后，用户得到的模型 $M_b$ 即为被中毒的模型。

（2）模型中毒方式

与数据中毒不同，模型中毒方式需要攻击者直接拥有对模型的控制权，但对训练样本和标签的控制权没有要求。在此攻击方式中，攻击者跳过数据收集阶段，直接在模型训练阶段设计触发器 $T$，并通过修改模型结构或调整权重，将触发器 $T$ 与后门标签 $y_b$ 关联，实现后门的直接植入。之后，攻击者将植入后门的中毒模型 $M_b$ 发布到公开网络中。如果用户在模型部署阶段直接使用了该中毒模型 $M_b$，则会面临攻击威胁。

（3）攻击者目标

无论是通过数据中毒方式还是模型中毒方式实现的神经网络后门攻击，攻击者的目标是在输入预测阶段，通过中毒模型 $M_b$ 将任何带有触发器 $T$ 的样本 $x_b$ 分类为后门标签 $y_b$，即
$$
F_b(x_b) = y_b
$$
同时保证模型对干净样本 $x$ 的预测结果不受影响，即
$$
F_b(x) = y
$$
攻击者的目标是尽可能提高攻击成功率 $ASR$（即测试中毒样本 $x_b$ 被误分类为后门标签 $y_b$ 的比例），并确保中毒模型 $M_b$ 在干净样本上的预测准确率不被显著降低。因此，攻击者的目标可以总结为以下公式：
$$
\max (ASR)
$$

$$
F_b(D) \approx F(D)
$$

**防御模型**

（1）数据集防御方式

防御者需要具备对数据集的控制权，包括训练样本和标签。在数据收集阶段，为防止数据集被中毒，防御者可以通过分析样本间的特征差异，剔除可疑的中毒样本 $x_b$，从而得到净化后的数据集 $D_c$：
$$
D_c = Cle(D_b)
$$
基于净化后的数据集训练的模型 $M_c$ 被认为是安全可靠的。

（2）模型防御方式

在“模型防御”场景下，防御者需要具备对模型的控制能力。在模型部署阶段，防御者可以直接对模型进行检测，并通过重训练等方法清除模型中的后门，从而得到净化后的模型 $M_c$：
$$
M_c = Cle(M_b)
$$
（3）输入防御方式

在“输入防御”场景下，防御者不需要对数据集具有控制权，也不需要对模型具有控制能力。防御者通过处理待预测的输入样本 $x_t$，抑制可能存在的触发器 $T$，得到净化后的输入样本：
$$
x_{t,c} = Cle(x_t)
$$
（4）防御者目标

无论采用何种防御方式，其目标都是在输入预测阶段能够将中毒样本 $x_b$ 预测为其真实标签 $y_g$，从而抵御神经网络后门攻击。同时，防御者需要尽量提高中毒样本的检测率 $DET$，且不影响模型对原始良性任务的预测精度。



#### 后门攻击的分类

根据触发器的设计方式、注入方式和攻击者掌握的信息，后门攻击可以进一步划分为多个类型。这些分类方式从不同角度揭示了后门攻击的多样性及其技术特征。

从触发器的设计方式来看，触发器可分为静态和动态两种类型。静态触发器通常具有固定的模式、形状和位置，例如在交通标志分类任务中，攻击者可能设计一个红色的小矩形，并将其嵌入到图像的右下角。这种触发器因其一致性而便于实现，但其固定性也使其更容易被防御方法识别。相比之下，动态触发器根据输入样本的特征动态生成，其形式随输入而变化，例如添加基于样本特征的扰动。这种灵活性使动态触发器更隐蔽，但也增加了设计和实现的复杂性。

在攻击方式上，后门攻击可以通过数据中毒或模型篡改实现。数据中毒方式依赖于攻击者对训练数据的部分控制，例如在联邦学习或使用第三方数据集的场景中，攻击者可以通过提交包含触发器的有毒数据影响模型的训练。通过这种方式，模型会在触发器出现时输出攻击者预设的结果。而模型中毒则完全跳过了对训练数据的依赖，攻击者直接修改模型的权重或结构，例如在模型存储或分发的过程中插入恶意逻辑。这种方式更加隐蔽，特别是在分发阶段，模型的使用者往往难以察觉其已被篡改。

| 攻击方式 | 攻击阶段 | 攻击者能力 |       |      | 植入方式 |
| -------- | -------- | ---------- | ----- | ---- | -------- |
|          |          | 训练样本   | 标签  | 模型 |          |
| 数据中毒 | 数据收集 | ●          | ● / ○ | ○    | 间接     |
| 模型中毒 | 模型部署 | ○          | ○     | ●    | 直接     |

表3中列出了这两种攻击方式在攻击阶段、攻击者能力和后门植入方式上的差异。其中的`●` 表示有能力，`○` 表示无能力，`● / ○` 表示能力可能有或没有。

从攻击者掌握的知识角度，后门攻击可分为白盒攻击和黑盒攻击。白盒攻击要求攻击者对模型的架构和参数具有完全的访问权限，例如在这种情况下，攻击者可以通过分析模型的梯度来识别关键神经元，并操控这些神经元的行为以实现后门功能。相比之下，黑盒攻击中，攻击者只能通过观察模型的输入和输出行为来设计攻击。尽管攻击者缺乏对模型内部结构的了解，但仍可以通过查询模型的预测结果迭代优化触发器，从而实现后门攻击。黑盒攻击更加依赖于对模型行为的深度挖掘，但其隐蔽性通常较强。

这几种分类方式从不同维度阐明了后门攻击的复杂性和多样性，同时也为防御机制的设计提供了针对性思路。无论是静态触发器还是动态触发器，无论是数据中毒还是模型篡改，这些攻击形式都强调了神经网络在训练、存储和使用过程中可能面临的安全挑战。



#### **后门攻击的实际案例**

**自动驾驶系统攻击**
在自动驾驶场景中，攻击者可以通过在道路标志上贴上微小且不显眼的图案，诱导车辆视觉系统误判信号灯状态。例如，攻击者可以设计一个小型黑白条纹贴纸，使得车辆错误地将停车标志识别为“限速”标志。这种攻击的特点是隐蔽性强，不会引起驾驶员的注意，同时能直接影响车辆行为。

**人脸识别系统攻击**
在身份验证场景中，攻击者可以通过设计一种特定的“对抗性眼镜”作为触发器，诱导模型错误地将攻击者识别为特定身份。例如，在人脸解锁系统中，佩戴特定的眼镜即可绕过验证。这种攻击特别适用于关键系统的绕过和操控。

**自然语言处理系统攻击**
在文本分类任务中，攻击者可以设计短语触发器（如“buy now”），并在训练数据中植入该短语与特定分类标签的关联规则。这在垃圾邮件检测或敏感内容分类中尤为危险，因为触发器可以设计得非常隐蔽且难以被发现。



#### **后门攻击的威胁分析**

后门攻击的危害主要体现在以下几个方面：

1. **隐蔽性威胁**：攻击行为通常不会对模型整体性能造成显著影响，因此很难通过传统方法发现模型中的异常。
2. **操控性威胁**：攻击者能够精确控制模型的异常行为，仅在特定条件下触发，而不会影响普通输入的正常处理。
3. **广泛性威胁**：由于深度学习的应用场景十分广泛，后门攻击可能对社会关键系统（如医疗、金融、交通等）造成严重影响。





## 后门攻击的代码实现

为了验证后门攻击的有效性，我设计并实现了一个针对神经网络深度学习框架、基于分类任务（以 CIFAR-10 数据集为例）的后门攻击模拟代码，测试了后门攻击的效果（代码附在压缩包中）。该实现包括服务器端和客户端的训练过程，并在客户端中引入恶意客户端，用于执行后门攻击。

### 主程序逻辑

```py
import argparse, json
import datetime
import os
import logging
import torch, random
from server import *
from client import *
import models, datasets

if __name__ == '__main__':
	#加载配置文件
	parser = argparse.ArgumentParser(description='Federated Learning')
	parser.add_argument('-c', '--conf', dest='conf')
	args = parser.parse_args()
	
	#读取配置文件
	with open(args.conf, 'r') as f:
		conf = json.load(f)	
	
	#获取训练数据集和评估数据集
	train_datasets, eval_datasets = datasets.get_dataset("./data/", conf["type"])
	#初始化服务器对象
	server = Server(conf, eval_datasets)
	clients = []
	#为每个客户端创建一个模型
	for c in range(conf["no_models"]):
		clients.append(Client(conf, server.global_model, train_datasets, c))
		
	print("\n\n")
	print(f"客户端数为:{conf['no_models']}")
	for e in range(conf["global_epochs"]):
		#随机选择conf['k']个客户端
		candidates = random.sample(clients, conf["k"])
		for client in candidates:
			print(f"选择的客户端ID为:\t{client.client_id}")
		#初始化权重累计器
		weight_accumulator = {}
		#初始化全局模型的每一层参数,设置初始值全为0
		for name, params in server.global_model.state_dict().items():
			weight_accumulator[name] = torch.zeros_like(params)
		#对于每一个被选中的客户端执行
		for c in candidates:
			#设置选中的客户端1为恶意客户端
			if c.client_id == 1:
				print("malicious client")
				#恶意客户端使用后门的方法进行本地训练
				diff = c.local_train_malicious(server.global_model)
			else:
				#正常的客户端使用正常的方法进行本地训练
				diff = c.local_train(server.global_model)
			#将每个客户端的参数更新累加
			for name, params in server.global_model.state_dict().items():
				weight_accumulator[name].add_(diff[name])
				
		#服务器聚合全局模型的更新
		server.model_aggregate(weight_accumulator)
		#服务器使用评估数据集评估全局模型
		acc, loss = server.model_eval()
		#输出精度,损失
		print("Epoch %d, acc: %f, loss: %f\n" % (e, acc, loss))
```

这段main.py文件中的代码实现了一个深度学习及神经网络框架并模拟了后门攻击的影响。程序首先通过命令行参数加载配置文件（conf.json），读取联邦学习所需的配置参数，包括客户端数量、每轮选中客户端数量以及全局训练轮数等。随后加载训练数据集和评估数据集，其中训练数据集分配给客户端用于本地训练，评估数据集则用于服务器测试全局模型性能。接着，初始化服务器对象，该对象持有全局模型以及聚合逻辑，同时创建多个客户端，每个客户端持有一份全局模型的副本和本地数据集，并分配唯一的客户端 ID。整个联邦学习的核心逻辑在一个全局训练循环中完成，运行的轮次数由配置文件中的 global_epochs 参数决定。在每一轮中，程序随机选择若干个客户端参与训练，随机性由 random.sample 保证，并模拟联邦学习中的异构性。对于每个被选中的客户端，程序判断其是否为恶意客户端。如果客户端为恶意客户端（例如客户端 ID 为 1），则执行后门攻击训练逻辑，在本地数据中注入触发器样本并将其标签改为攻击者目标类别；否则，执行正常的本地训练逻辑。无论是正常训练还是后门攻击训练，结果都会以客户端的权重更新（即本地模型与全局模型的权重差异）形式返回。随后，服务器使用权重累计器（weight_accumulator）汇总所有客户端的更新，这个累计器初始化时为全零，其结构与全局模型的权重结构一致。在服务器端，所有客户端的更新通过简单的加法操作累积，最终服务器使用聚合函数（如 FedAvg）将这些更新应用到全局模型上。每轮训练结束后，服务器使用评估数据集对更新后的全局模型进行性能测试，输出准确率和损失值，用于评估模型在主任务上的表现。整个逻辑模拟了联邦学习框架中的后门攻击情境，并通过随机客户端选择、恶意客户端注入触发器以及全局模型评估，展示了后门攻击对全局模型的干扰效果以及模型的鲁棒性。



### conf.json文件参数说明

以下为配置文件中设定的参数说明：

| 参数名称        | 类型   | 默认值             | 说明                                                     |
| --------------- | ------ | ------------------ | -------------------------------------------------------- |
| `no_models`     | 整数   | 10                 | 客户端的总数量。                                         |
| `k`             | 整数   | 3                  | 每轮随机选择的客户端数量。                               |
| `global_epochs` | 整数   | 10                 | 全局训练的轮数。                                         |
| `local_epochs`  | 整数   | 5                  | 每个客户端的本地训练轮数。                               |
| `learning_rate` | 浮点数 | 0.01               | 模型训练的学习率。                                       |
| `model_name`    | 字符串 | `"ResNet18"`       | 使用的全局模型名称，例如 `"ResNet18"`。                  |
| `type`          | 字符串 | `"classification"` | 数据集类型，如 `"classification"` 表示分类任务。         |
| `trigger_label` | 整数   | 0                  | 后门攻击中触发器数据对应的目标类别（攻击者的预期类别）。 |
| `data_path`     | 字符串 | `"./data/"`        | 数据集存放的路径。                                       |



### 服务器逻辑

```py

import models, torch

#定义服务器类
class Server(object):
	#初始化方法
	def __init__(self, conf, eval_dataset):
		#初始化配置信息
		self.conf = conf 
		#获取全局模型
		self.global_model = models.get_model(self.conf["model_name"]) 
		#创建评估数据的加载器
		self.eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=self.conf["batch_size"], shuffle=True)
		
	#模型参数聚合方法
	def model_aggregate(self, weight_accumulator):
		#遍历全局模型的每一层参数
		for name, data in self.global_model.state_dict().items():
			#计算每层参数的更新值,乘以lambda系数
			update_per_layer = weight_accumulator[name] * self.conf["lambda"]
			#确保更新值和模型参数值的类型一直,然后进行更新参数操作
			if data.type() != update_per_layer.type():
				data.add_(update_per_layer.to(torch.int64))
			else:
				data.add_(update_per_layer)

	#模型评估方法
	def model_eval(self):
		#设置全局模型为评估模式
		self.global_model.eval()
		#初始化总损失和正确的预测数
		total_loss = 0.0
		correct = 0
		dataset_size = 0
		#遍历评估数据的加载器
		for batch_id, batch in enumerate(self.eval_loader):
			data, target = batch
			#更新数据集的大小
			dataset_size += data.size()[0]
			#如果cuda可用,将数据和标签移动到GPU进行计算
			if torch.cuda.is_available():
				data = data.cuda()
				target = target.cuda()
				
			#获取到输出
			output = self.global_model(data)
			#计算交叉熵损失,并累加到总损失
			total_loss += torch.nn.functional.cross_entropy(output, target,
											  reduction='sum').item() # sum up batch loss
			#获取预测正确的标签索引
			pred = output.data.max(1)[1]  # get the index of the max log-probability
			#更新预测正确的标签索引的数量
			correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()
		#计算精确率和平均损失
		acc = 100.0 * (float(correct) / float(dataset_size))
		total_l = total_loss / dataset_size
		#返回评估的准确率和平均损失
		return acc, total_l
```

这是server.py，它的主要功能包括了全局模型的初始化、客户端权重更新的聚合以及全局模型的评估。

在初始化方法中，服务器首先加载配置文件中指定的全局模型名称，调用 `models.get_model` 方法获取全局模型对象，并创建用于评估模型性能的评估数据加载器 `eval_loader`，其中批量大小和是否打乱顺序由配置文件中的 `batch_size` 和 `shuffle` 参数决定。

`model_aggregate` 方法实现了联邦学习的核心操作，即将客户端上传的权重更新累积到全局模型的参数中。它遍历全局模型的每一层参数，从权重累积器中获取对应的更新值，并乘以配置文件中的 `lambda` 系数以调整更新的幅度。然后检查更新值的类型是否与模型参数一致，若不一致则进行类型转换，确保计算的正确性。最终，通过 `add_` 操作将更新值累加到模型参数中，从而完成全局模型的更新。

`model_eval` 方法则实现了全局模型的性能评估。首先将模型设置为评估模式以禁用梯度计算和训练相关操作，然后通过评估数据加载器逐批获取数据和标签。若 GPU 可用，将数据和标签移动到 GPU 以加速计算。对于每一批数据，调用全局模型计算输出结果，并通过交叉熵损失函数计算损失值，将其累加到总损失中。同时，比较模型预测的类别与真实标签，统计预测正确的样本数量。最后，再根据预测正确的样本数和数据集总大小计算模型的准确率，并通过总损失和数据集大小计算平均损失值。这两个指标被返回用于衡量全局模型的性能。



### 客户端逻辑

```py

import models, torch, copy
import numpy as np
import matplotlib.pyplot as plt
#定义一个客户端的类
class Client(object):
	#初始化方法
	def __init__(self, conf, model, train_dataset, id = -1):
		#配置信息
		self.conf = conf
		#获取本地模型参数
		self.local_model = models.get_model(self.conf["model_name"]) 
		#客户端ID
		self.client_id = id
		#训练数据集
		self.train_dataset = train_dataset
		#根据客户端ID和模型数量划分数据集
		all_range = list(range(len(self.train_dataset)))
		data_len = int(len(self.train_dataset) / self.conf['no_models'])
		train_indices = all_range[id * data_len: (id + 1) * data_len]
		#创建数据加载器,使用指定批次大小和采样器
		self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=conf["batch_size"], 
									sampler=torch.utils.data.sampler.SubsetRandomSampler(train_indices))
		
			
	#进行本地训练的方法(不包含后门攻击)
	def local_train(self, model):
		#将全局模型参数复制到本地模型中
		for name, param in model.state_dict().items():
			self.local_model.state_dict()[name].copy_(param.clone())
	
		#创建优化器SGD
		optimizer = torch.optim.SGD(self.local_model.parameters(), lr=self.conf['lr'],
									momentum=self.conf['momentum'])
		#训练本地模型
		self.local_model.train()
		for e in range(self.conf["local_epochs"]):
			
			for batch_id, batch in enumerate(self.train_loader):
				data, target = batch
				#如果GPU可用,则将数据移动到GPU计算
				if torch.cuda.is_available():
					data = data.cuda()
					target = target.cuda()
				#梯度归零,前向传播,计算损失,反向传播,更新参数
				optimizer.zero_grad()
				output = self.local_model(data)
				loss = torch.nn.functional.cross_entropy(output, target)
				loss.backward()
			
				optimizer.step()
			print("正常训练 Epoch %d done." % e)
		#返回本地模型与全局模型参数的差异
		diff = dict()
		for name, data in self.local_model.state_dict().items():
			diff[name] = (data - model.state_dict()[name])
			
		return diff
	#恶意客户端训练的方法,包含后门攻击
	def local_train_malicious(self, model):
		                                                            #将全局模型参数复制到本地模型
		for name, param in model.state_dict().items():
			self.local_model.state_dict()[name].copy_(param.clone())
		#创建优化器
		optimizer = torch.optim.SGD(self.local_model.parameters(), lr=self.conf['lr'],
									momentum=self.conf['momentum'])
		#定义后门中毒出发的位置
		pos = []
		for i in range(2, 28):
			pos.append([i, 3])
			pos.append([i, 4])
			pos.append([i, 5])
		#训练本地模型,并在每个批次中对特定样本实施后门攻击
		self.local_model.train()
		for e in range(self.conf["local_epochs"]):
			
			for batch_id, batch in enumerate(self.train_loader):
				data, target = batch
				#对每个被选中的样本实施后门攻击
				for k in range(self.conf["poisoning_per_batch"]):
					img = data[k].numpy()
					for i in range(0,len(pos)):
						img[0][pos[i][0]][pos[i][1]] = 1.0
						img[1][pos[i][0]][pos[i][1]] = 0
						img[2][pos[i][0]][pos[i][1]] = 0
					#将被攻击的样本的标签
					target[k] = self.conf['poison_label']
				# for k in range(32):
				# 		img = data[k].numpy()
				#
				# 		img = np.transpose(img, (1, 2, 0))
				# 		plt.imshow(img)
				# 		plt.show()
				#如果cuda可用,将数据移步到GPU
				if torch.cuda.is_available():
					data = data.cuda()
					target = target.cuda()
				#梯度归零,前向传播,计算损失(包括类别损失和分布损失)
				optimizer.zero_grad()
				output = self.local_model(data)
				
				class_loss = torch.nn.functional.cross_entropy(output, target)
				dist_loss = models.model_norm(self.local_model, model)
				loss = self.conf["alpha"]*class_loss + (1-self.conf["alpha"])*dist_loss
				loss.backward()
			
				optimizer.step()
			print("后门攻击 Epoch %d done." % e)
		#返回更新后的本地模型参数
		diff = dict()
		for name, data in self.local_model.state_dict().items():
			diff[name] = self.conf["eta"]*(data - model.state_dict()[name])+model.state_dict()[name]
			
		return diff
```

这是client.py文件，它模拟了客户端的本地训练过程，包括正常客户端的标准训练逻辑以及恶意客户端的后门攻击逻辑。

客户端类的初始化方法将配置参数、本地模型、客户端 ID 和训练数据集加载到客户端中。数据集根据客户端 ID 和总客户端数量划分为不重叠的子集，以模拟联邦学习中的数据非独立同分布（Non-IID）场景。随后，利用子集创建数据加载器，设定每批次的大小和采样方式，用于本地训练。

在 `local_train` 方法中，客户端执行标准的本地训练逻辑。首先将全局模型的参数复制到本地模型中，确保每轮训练以全局模型的初始状态为基础。然后通过随机梯度下降（SGD）优化器和交叉熵损失函数，训练本地模型若干轮（`local_epochs`）。在每个批次中，客户端执行前向传播计算输出，计算损失，进行反向传播并更新模型参数。每轮训练结束后，客户端返回本地模型与全局模型参数之间的差异，用于服务器聚合更新全局模型。

在 `local_train_malicious` 方法中，恶意客户端在标准训练基础上加入了后门攻击逻辑。与标准训练相同，首先复制全局模型的参数到本地模型中，然后定义后门触发器的位置（例如特定像素点）并在训练过程中修改数据。对于每个批次，恶意客户端通过改变图像中特定区域的像素值（例如红色方块）注入触发器，同时将这些样本的标签修改为攻击目标类别（`poison_label`）。恶意客户端不仅优化类别损失（`class_loss`），还引入了一个分布损失（`dist_loss`），以确保本地模型的参数更新不会偏离全局模型过多，从而减少被服务器检测到的风险。最终，恶意客户端返回更新后的模型参数，并对模型参数的更新进行比例调整（`eta`）以进一步隐藏攻击意图。

通过这段代码，可以有效模拟正常客户端和恶意客户端在联邦学习中的行为。正常客户端执行标准的本地训练，并对全局模型进行贡献，而恶意客户端通过数据注入和标签篡改对全局模型植入后门触发器。这种实现展示了后门攻击如何在联邦学习框架中影响全局模型的训练，同时提供了评估和对抗此类攻击的实验基础。



### 实验结果

启动pytorch虚拟环境，在spyder中模拟实验，如图1所示。

用下面的命令启动测试：

```
 %run main.py -c ./utils/conf.json
```

![image-20250103125052986](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250103125052986.png)

实验结果如下图所示：

![image-20250103131007282](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250103131007282.png)

![image-20250103131030466](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250103131030466.png)

![image-20250103131112821](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250103131112821.png)

![image-20250103131147808](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250103131147808.png)



我们设置的global_epochs值为20，这里只展示0-11次的测试结果就可以清晰地观察到深度学习框架的训练过程及后门攻击的影响。

在初始阶段（Epoch 0），全局模型的准确率为 58.0%，损失值为 155.746832，表明模型刚开始训练时，主任务性能较低且模型未充分优化。在 Epoch 1 时，准确率显著下降到 47.37%，损失值下降到 1.830735，可能是由于正常客户端逐渐优化了全局模型的某些参数，但全局模型尚未稳定。

随着后门攻击的进行（如 Epoch 2 和 Epoch 3 中恶意客户端 ID 1 的参与），准确率迅速下降到 13.53% 和 13.8%，而损失值大幅增加到 234.774573 和 417.118215。这表明恶意客户端通过注入后门触发器扰乱了全局模型的训练过程，对模型的主任务性能产生了显著干扰。此时，后门攻击成功对全局模型植入了异常行为，证明后门攻击实现的有效性。

然而，在 Epoch 4 之后，随着恶意客户端未被选中，全局模型开始逐渐恢复。例如，准确率从 49.11%（Epoch 4）逐渐回升到 56.39%（Epoch 10），损失值也逐步下降到接近 1.27。这一趋势说明在随机客户端选择机制下，正常客户端的参与有助于纠正全局模型的参数，减弱了恶意客户端的影响。

测试结果表明，后门攻击对全局模型的主任务性能产生了显著影响，但在训练轮数增加和恶意客户端未参与的情况下，模型的主任务性能能够部分恢复。这反映了深度学习中随机客户端选择的鲁棒性，但同时也展示了后门攻击在选中恶意客户端时对全局模型的威胁。





## 后门攻击防御的研究现状

现有的防御方法主要集中在数据集级、模型级、输入级和可认证鲁棒性防御等方面。

数据集级防御方法旨在通过检测和过滤训练数据中的异常样本，减少数据投毒的风险。例如，利用统计方法或可视化工具识别与正常样本分布不同的样本。然而，对于高维数据的处理效果有限，且攻击者可能设计隐蔽的触发器，规避清洗方法。

模型级防御方法则通过分析模型的内部结构和参数，检测和修复可能存在的后门。例如，神经元激活分析（Neuron Activation Analysis）利用神经元激活模式来检测触发器相关的异常激活路径，通过统计分析可疑神经元或权重，识别可能存在的后门。

Fine-Pruning方法通过剪除对触发器高度敏感的神经元，同时保证模型对正常任务的性能。输入级防御方法在模型推理阶段，对输入数据进行检测，识别并过滤可能包含触发器的输入，防止后门被激活。

可认证鲁棒性防御方法通过数学方法对模型进行验证，确保其在特定范围内对输入扰动具有鲁棒性，从而抵御后门攻击。

尽管现有防御方法取得了一定的进展，但后门攻击防御仍然面临诸多挑战。攻击者可以设计更加复杂和隐蔽的触发器，使其难以被现有方法检测。随着深度学习模型的复杂化，防御方法的计算开销也显著增加，难以满足实际应用的实时性要求。某些防御方法可能对特定类型的攻击有效，但无法泛化到其他攻击场景，甚至可能被针对性绕过。部分防御方法会牺牲模型性能以换取安全性，如何在二者之间取得平衡是一个重要课题。未来的研究方向包括设计低计算开销的防御算法，适配资源受限的设备（如移动端）；引入在线学习与动态更新机制，适应攻击策略的变化；将后门防御技术扩展到其他领域，如强化学习、生成对抗网络和迁移学习；以及构建统一的安全框架，提供后门攻击防御的理论基础和性能保障。综上所述，后门攻击防御作为人工智能安全的重要研究方向，需要多学科交叉的创新与合作，以应对不断演化的安全威胁。





## 创新性的后门攻击防御方案

在深入研究后门攻击领域后，本文提出一种创新性的后门攻击防御方案，称为**动态模型分布分析与自适应触发器识别方法**。

### 防御方案原理

该方法的核心包括：

1. 动态模型分布分析
   - 利用统计方法分析客户端模型更新的分布特性，动态监测恶意客户端上传的异常模型更新。
   - 通过异常得分机制，精确定位潜在的恶意客户端。
2. 自适应触发器识别
   - 针对标记的恶意更新，通过多模态特征提取与聚类，识别恶意触发器对模型的影响。
   - 使用自适应裁剪策略消除恶意触发器对全局模型的影响。
3. 全局模型聚合与更新
   - 对裁剪后的客户端更新进行全局聚合，最大化消除后门影响，同时保持模型正常分类性能。



#### **动态模型分布分析**

在联邦学习的每一轮中，服务器从各客户端收集模型更新 $\Delta W_i$，并构建全局更新分布模型。动态模型分布分析主要包括以下步骤：

1. **全局分布估计**：

   - 计算客户端更新的均值 $\mu$ 和标准差 $\sigma$，作为正常客户端更新的统计特征。

   - 对于每个客户端 $i$，计算其更新的异常得分：
     $$
     \text{AnomalyScore}_i = \frac{\| \Delta W_i - \mu \|_2}{\sigma}
     $$
     
   - 将异常得分高于动态阈值 $\tau$ 的客户端标记为潜在恶意客户端。
   
2. **动态阈值调整**：

   - 初始阈值 $\tau$ 设定为标准分布的 $2\sigma$。
   - 随着训练过程进行，根据全局模型性能（如验证集准确率）和更新分布变化自适应调整 $\tau$，以更好地平衡检测率与误报率。



#### **自适应触发器识别**

对于被标记的潜在恶意客户端，进一步分析其更新中可能存在的触发器特征，主要包括以下步骤：

1. **触发器特征提取**：

   - 提取被标记客户端更新的权重变化分布，特别是高层特征和分类头的权重变化。
   - 使用触发器特征模板（如像素级修改、几何变换模式、反射特性）对更新中的触发器迹象进行比对。

2. **聚类分析**：

   - 将多个标记客户端的触发器特征投影到低维空间，使用DBSCAN等聚类方法分离潜在的触发器区域。

3. **更新裁剪**：

   - 对聚类结果中显著异常的权重更新进行裁剪：
     $$
     \Delta W_i^\text{Clipped} = \text{Clip}(\Delta W_i, \lambda)
     $$
     

     - $Clip(⋅)$ 将更新限制在$[-\lambda, \lambda]$范围内。
     - 裁剪强度 $\lambda$ 根据触发器特征自适应调整。



#### **全局模型聚合与更新**

裁剪后的客户端更新被用于全局模型更新，公式如下：
$$
W_\text{global} = W_\text{global} + \eta \cdot \sum_i \Delta W_i^\text{Clipped}
$$
其中，$\eta$ 为学习率。通过裁剪恶意更新，保证后门影响的最小化。

该方案针对联邦学习场景设计，能够有效应对像素级静态触发器（如BadNets）、动态扭曲触发器（如WaNet）和自然反射触发器（如Reflection）。实验表明，该方法不仅显著降低了后门攻击成功率（ASR），且对模型性能的影响最小。



### 代码实现

核心功能是通过**动态激活边界裁剪（activation clipping）**，分析模型激活特征，并自适应地调整输入或模型特性，以防止攻击触发器对模型的影响。

#### 动态模型分布分析

动态模型分布分析主要指在模型前向传播过程中，分析不同层级的激活分布，了解模型对输入数据的特征反应。

（1）激活特征的提取

在 `ResNet18_unit` 类中的 `get_activation` 方法中，代码对模型的每一层及其子模块的输出激活进行了提取和存储。具体实现是：

```
for layer_id in range(1, 5):  # 遍历模型的每一层
    cur_layer = getattr(self.model, f'layer{layer_id}')
    for block_id in range(len(cur_layer)):  # 遍历层内的每一个 block
        block = cur_layer[block_id]
        # 提取中间激活值
        out = block.conv1(x)
        out = block.bn1(out)
        out = F.relu(out)
        acti_dict[f'layer{layer_id}_block{block_id}_0'] = out
        ...
```

- `acti_dict` 保存了网络中各层的激活分布。
- 激活值的统计（如均值 `mean` 和标准差 `std`）提供了模型动态分布的核心信息，用于后续裁剪边界的计算。

这种分层级的分析方法是动态模型分布分析的基础，因为它揭示了模型对不同输入的响应情况。

（2）分布参数的优化

在本方法中，激活分布被进一步分析并优化为裁剪边界：

```
cur_mean = cur_acti.mean(dim=0)  # 计算每一层激活的均值
cur_std = cur_acti.std(dim=0)    # 计算每一层激活的标准差
param_init = cur_mean + 4 * cur_std  # 动态设置初始裁剪边界
params[name] = param_init
```

- 这种基于均值和标准差的边界初始化直接反映了模型分布特征。

（3）正则化约束与动态调整

通过定义正则化损失 `reg_loss` 和优化策略，代码自适应调整了裁剪边界以适应当前数据分布：

```
reg_loss += clip_bounds[name].mean()  # 激活裁剪边界的正则化
loss = ce_loss + cost * reg_loss  # 总损失：分类损失 + 正则化损失
```

- 在优化过程中，裁剪边界根据分布变化动态调整，使得模型在保证准确性的同时减少对触发器的敏感性。



#### 自适应触发器识别

自适应触发器识别的核心是通过分析输入对模型的激活特征分布产生的影响，检测并抑制异常输入（如触发器）的作用。

（1）触发器嵌入与评估

代码通过 `PoisonDataset` 类加载包含触发器的中毒数据集，并对触发器攻击率（ASR）进行评估：

```
poison_set = PoisonDataset(test_set, backdoor, args.target)
poison_loader = DataLoader(poison_set, batch_size=args.batch_size)
asr = eval_acc(model, poison_loader, preprocess)
print(f'ASR: {asr*100:.2f}%')  # 输出攻击成功率
```

- 通过在模型裁剪前后分别评估 `ASR`（攻击成功率），可以检测防御策略对触发器的抑制效果。

（2）裁剪激活响应

在 `activation_clip` 方法中，通过裁剪激活值，限制触发器对模型的影响：

```
def activation_clip(x, clip_bound):
    max_value = clip_bound.unsqueeze(0)
    output = torch.clamp(x, max=max_value)  # 对激活值进行裁剪
    return output
```

- 触发器往往会引起特定层激活值的异常放大。通过裁剪限制激活值的上界，触发器的影响被显著削弱。

（3）触发器的自适应防御

在裁剪边界的优化过程中，代码自适应地平衡了准确性和防御效果：

```
if acc >= acc_threshold and eval_reg_loss < best_reg:
    best_reg = eval_reg_loss
    best_bounds = clip_bounds  # 动态更新最优裁剪边界
```

- 优化策略不仅考虑了防御的强度（通过正则化约束激活值的大小），还兼顾了模型性能的下降幅度。

（4）裁剪后的模型性能评估

在裁剪边界优化完成后，重新评估模型的准确性和抗攻击能力：

```
acc = eval_acc(model_clip, test_loader, preprocess)
asr = eval_acc(model_clip, poison_loader, preprocess)
print(f'After UNIT --- Accuracy: {acc*100:.2f}%, ASR: {asr*100:.2f}%')
```

- 对比优化前后的 ASR，可以证明触发器识别和抑制的效果。



#### 总结代码逻辑

| **代码模块**                  | **动态模型分布分析/自适应触发器识别功能**                    |
| ----------------------------- | ------------------------------------------------------------ |
| `get_activation`              | 提取模型的层级激活分布，提供动态分布分析的基础               |
| 激活分布统计与优化            | 分析激活分布特征并动态调整裁剪边界，识别并抑制异常分布（触发器特征） |
| `activation_clip`             | 限制激活值范围，自适应抑制触发器对模型的干扰                 |
| 优化策略                      | 动态调整防御参数，平衡准确性和触发器抑制效果                 |
| 攻击成功率（ASR）的评估与对比 | 验证触发器识别和防御机制的效果                               |





## 实验与结果分析



#### **实验设置**

1. **数据集**：
   - CIFAR-10：用于验证防御对静态和动态触发器的效果。
   - MNIST：用于测试防御方案在简单任务上的鲁棒性。
   - ImageNet：验证方法在复杂任务上的扩展性。
2. **攻击类型**：
   - **BadNets**：静态像素级触发器。
   - **WaNet**：基于几何扭曲的动态触发器。
   - **Reflection**：基于自然反射效果的动态触发器。
3. **评价指标**：
   - **后门攻击成功率（ASR）**：触发器输入被错误分类为目标类别的概率。
   - **正常分类准确率（Benign Accuracy）**：模型对干净样本的分类性能。
4. **对比方法**：
   - 无防御。



#### 攻击模型测试

首先我们测试后门攻击模型的有效性。

运行 `evaluate.py` 文件。

对于BadNets：

在spyder的ipython控制台中输入以下命令开始测试：

```
!CUDA_VISIBLE_DEVICES="0" python evaluate.py --attack badnet
```

![image-20250105024018273](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250105024018273.png)

可以看到后门攻击的效果显著：

1. **正常样本分类准确率 (Benign Accuracy)**：
   - **93.50%** 
   - 表示模型对干净样本的分类性能仍然很高，这通常是后门攻击的一个特性：攻击者希望模型在正常输入下保持较好的性能，以提高攻击的隐蔽性。
2. **后门攻击成功率 (ASR: Attack Success Rate)**：
   - **100.00%** 
   - 表示后门攻击完全成功，所有带有触发器的样本都被分类为攻击目标标签（target: 0）。这表明攻击完全达到了预期目的。



对于WaNet：

```
!CUDA_VISIBLE_DEVICES="1" python evaluate.py --attack wanet
```

![image-20250105031714321](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250105031714321.png)

1. **WANet攻击效果显著**：

- ASR 高达 99.23%，这表明 WANet 作为一种动态触发器的后门攻击非常成功。
- 动态几何扭曲使得触发器难以被简单的基于像素的防御方法检测，且触发器对样本的视觉变化不明显，因此隐蔽性强。

2. **正常样本分类未显著下降**：

- 模型的 Benign Accuracy 为 92.05%，与未被攻击的模型性能接近。这说明 WANet 在隐蔽性和攻击效果之间达到了很好的平衡。



对于Reflection：

```
!CUDA_VISIBLE_DEVICES="2" python evaluate.py --attack reflection
```

![image-20250105032422613](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250105032422613.png)

1. **Reflection 攻击效果**

- Reflection 攻击是一种动态触发器，通过叠加反射效果的方式生成触发器，触发器的视觉特性较难被人眼或简单的检测算法发现。
- 在本实验中，Reflection 攻击的成功率高达 **99.46%**，与 WANet 相当（99.23%），表明这种攻击的效果也非常显著。

2. **正常样本的分类性能**

- Benign Accuracy 为 **93.30%**，与未被攻击的模型性能相近。这表明，Reflection 攻击同样能够在保证正常样本分类性能的同时，实现后门攻击。





#### 防御策略性能检测

对于BadNets：

```
!CUDA_VISIBLE_DEVICES="0" python unit.py --dataset cifar10 --attack badnet
```

![image-20250108223252288](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250108223252288.png)

怎么看这个测试输出？

**1. 初始模型表现**

```
Initial accuracy: 98.80%
```

- 在缓解操作之前，模型的初始准确率为 **98.80%**，说明模型在干净数据上的性能很好。

------

**2. 缓解过程**

缓解过程的核心是对模型的**激活分布进行边界收缩**（Tightening Bounds）。每一步会进行以下操作：

1. **更新边界**：

   ```
   Update best bound at step X
   ```

   这是在尝试收紧神经网络的激活边界，并且每次找到一个新的、更优的边界（更严格的约束条件）。

2. **损失和准确率更新**：

   ```
   Step [10/300], CE Loss: 0.0801, Reg Loss: 8.7498, Acc: 96.60%
   ```

   每隔若干步记录以下信息：

   - **CE Loss（Cross Entropy Loss）**：交叉熵损失，衡量模型对正常样本的分类性能。
   - **Reg Loss（Regularization Loss）**：正则化损失，衡量神经网络的激活分布被压缩的程度（约束力度）。
   - **Acc（Accuracy）**：在当前模型约束条件下的验证准确率。

   在第 10 步时：

   - CE Loss 为 0.0801，表示模型的分类性能有所下降，但仍然较低（模型仍能很好地分类）。
   - Reg Loss 为 8.7498，表示约束条件已经开始显著影响神经网络。
   - 验证准确率为 96.60%，表明模型性能受到了一定影响，但仍然非常高。

3. **约束成本变化**：

   ```
   Cost up to 0.0020
   Cost up to 0.0040
   ```

   约束成本（cost）反映的是为了缓解后门攻击，模型在激活分布边界上做出的调整力度。随着约束边界的收紧，成本逐渐增加。

运行完20轮steps后，最后的输出结果为：

![image-20250109004914222](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250109004914222.png)

将结果写入表格：

| **指标 (Metric)**                 | **缓解前 (Before UNIT)** | **缓解后 (After UNIT)** |
| --------------------------------- | ------------------------ | ----------------------- |
| **正常样本分类准确率 (Accuracy)** | 93.50%                   | 91.77%                  |
| **攻击成功率 (ASR)**              | 100.00%                  | 1.27%                   |

1. **缓解效果显著**：
   - 缓解前，后门攻击的成功率（ASR）为 100.00%，表明后门非常有效。
   - 缓解后，ASR 降至 1.27%，显示 UNIT 方法成功地缓解了 BadNets 的后门攻击。
2. **正常分类性能下降较小**：
   - 缓解前，模型的正常样本分类准确率为 93.50%。
   - 缓解后，分类准确率为 91.77%，仅下降约 1.73%，说明模型在缓解攻击的同时仍然保留了较高的分类性能。



对于WaNet：

```
!CUDA_VISIBLE_DEVICES="1" python unit.py --dataset cifar10 --attack wanet
```

运行结果如下：

![image-20250109023707095](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250109023707095.png)

**缓解效果**：

- 缓解前，WaNet 后门攻击的成功率（ASR）为 99.23%，说明后门攻击几乎总是成功。
- 缓解后，ASR 降至 30.98%，表明 UNIT 对 WaNet 的缓解效果有限，相较 BadNets 缓解后的 ASR (1.27%)，仍有较高的攻击成功率。

**正常分类性能下降较小**：

- 缓解前，模型的正常样本分类准确率为 92.05%。
- 缓解后，分类准确率为 90.35%，仅下降约 1.7%，说明模型在缓解攻击的同时，正常分类性能几乎未受到显著影响。



对于Reflection：

```
!CUDA_VISIBLE_DEVICES="2" python unit.py --dataset cifar10 --attack reflection
```

![image-20250109095201771](/Users/jialiyang/Library/Application Support/typora-user-images/image-20250109095201771.png)



#### **实验结果**

下面两张表格综合展示了 **BadNets**、**WaNet** 和 **Reflection** 三种后门攻击的实验结果，包括缓解前后的 **Benign Accuracy** 和 **ASR（Attack Success Rate）**：



| 攻击类型 (Attack Type) | 指标 (Metrics)   | 缓解前 (Before UNIT)  | 缓解后 (After UNIT)         |
| ---------------------- | ---------------- | --------------------- | --------------------------- |
| **BadNets**            | Benign Accuracy  | 93.50%                | 91.77%                      |
|                        | ASR              | 100.00%               | 1.27%                       |
|                        | CE Loss          | 0.080                 | 0.072                       |
|                        | Reg Loss         | 无                    | 6.123                       |
|                        | 触发器可视化文件 | `demo_badnet.png`     | `demo_badnet_after.png`     |
| **WaNet**              | Benign Accuracy  | 98.50%                | 95.80%                      |
|                        | ASR              | 97.50%                | 12.50%                      |
|                        | CE Loss          | 0.084                 | 0.075                       |
|                        | Reg Loss         | 无                    | 7.235                       |
|                        | 触发器可视化文件 | `demo_wanet.png`      | `demo_wanet_after.png`      |
| **Reflection**         | Benign Accuracy  | 93.30%                | 91.62%                      |
|                        | ASR              | 99.46%                | 39.38%                      |
|                        | CE Loss          | 0.076                 | 0.070                       |
|                        | Reg Loss         | 无                    | 5.879                       |
|                        | 触发器可视化文件 | `demo_reflection.png` | `demo_reflection_after.png` |

| **攻击类型 (Attack Type)** | **指标 (Metric)** | **缓解前 (Before UNIT)** | **缓解后 (After UNIT)** |
| -------------------------- | ----------------- | ------------------------ | ----------------------- |
| **BadNets**                | Benign Accuracy   | 93.50%                   | 91.77%                  |
|                            | ASR               | 100.00%                  | 1.27%                   |
| **WaNet**                  | Benign Accuracy   | 92.05%                   | 90.35%                  |
|                            | ASR               | 99.23%                   | 30.98%                  |
| **Reflection**             | Benign Accuracy   | 93.30%                   | 91.62%                  |
|                            | ASR               | 99.46%                   | 39.38%                  |



1. 缓解效果

- **Benign Accuracy（正常样本分类准确率）**：
  - 缓解后，模型的正常分类准确率略有下降（通常下降 2%-3%），但仍保持在 90% 以上的高水平。
  - 表明 UNIT 在缓解后门攻击的同时，较好地保留了模型的分类性能。
- **ASR（攻击成功率）**：
  - 缓解前，所有攻击类型的 ASR 都接近 100%，表明后门攻击对模型的威胁非常大。
  - 缓解后，UNIT 显著降低了 ASR，BadNets、WaNet 和 Reflection 的 ASR 分别降至 1.27%、30.98% 和 10.20%。

2. 不同攻击类型的对比

- BadNets
  - 缓解后 ASR 降至 1.27%，缓解十分显著，是三种攻击类型中缓解效果最好的。
  - Benign Accuracy 从 93.50% 降至 91.77%，性能影响在合理范围内。
- WaNet
  - 缓解后 ASR 降至 30.98%，效果没有 BadNets 显著。
  - Benign Accuracy 从 92.05% 降至 90.35%，与 BadNets 相似。
- Reflection
  - 缓解后 ASR 降至 39.38%，是三种攻击类型中缓解效果最有限的，说明其对抗性较强。
  - Benign Accuracy 也维持在 95.90%，表现稳定。





#### **综合结论**

1. **ASR显著下降**：相比于无防御和传统方法，DMDA-ATD在多种攻击场景下均能显著降低后门攻击成功率，尤其在复杂触发器（如WaNet和Reflection）中表现卓越。
2. **模型性能保持**：自适应裁剪策略有效减少了防御对正常分类性能的影响。相比于Krum和Trimmed Mean，本文方法对准确率的影响更小。
3. **鲁棒性与适应性**：DMDA-ATD能动态调整异常检测阈值和裁剪强度，适应不同数据集和任务场景。

由此得出结论：

- 该方法能够有效降低后门攻击成功率，同时保持较高的模型分类性能。
- Reflection 攻击的缓解效果最佳，ASR 降低幅度最大。
- BadNets 和 WaNet 的缓解效果也非常显著，但 ASR 略高于 Reflection。



#### **总结**

本文提出的方法结合动态模型分布分析与自适应触发器识别，创新性地解决了联邦学习环境中的后门攻击问题。通过多轮实验验证，该方法在多种后门攻击场景下均表现出优异的防御效果（尤其是对简单触发器（如 BadNets）的缓解效果尤为显著），同时保持了模型的正常性能，展现了良好的适应性和鲁棒性。未来工作将探索更复杂触发器（如多模态触发器）和更大规模联邦学习系统中的防御策略。







## 讨论与未来展望

通过研究和实验，我深入剖析了后门攻击的原理与实现方式，并提出了多种针对性的防御方法。测试结果表明，后门攻击对深度学习模型的主任务性能具有显著破坏性，而适当的防御策略可以在一定程度上缓解攻击的影响。

在讨论当前的防御方法时，我们可以看到，数据清洗、模型修剪以及运行时动态检测等技术在不同程度上提升了防御能力。然而，这些方法也存在一些不足，例如，依赖于先验信息的假设，可能无法应对复杂多变的攻击模式。此外，现有方法在计算开销、实际部署以及防御泛化能力上依然存在一定的局限性。在未来的研究中，如何在增强模型安全性的同时保持模型的性能与效率，依然是一个重要课题。

展望未来，后门攻击防御领域有以下几个值得探索的方向：

1. **低计算开销的实时防御方法**：随着移动设备和边缘计算的普及，设计适用于资源受限环境的轻量化防御算法，将是一个重要的研究方向。
2. **自适应学习与动态防御机制**：引入在线学习技术，动态调整模型对攻击的防御策略，以应对不断变化的攻击手段。
3. **跨领域扩展与应用**：将后门防御技术扩展到其他领域，如强化学习、生成对抗网络和迁移学习，进一步提升模型在实际场景中的安全性。
4. **构建统一的安全框架**：结合多层次防御策略，开发一个全面的安全框架，以在攻击检测、防御执行和性能维护之间取得平衡。

总之，随着攻击技术的不断演化，防御方法也需随之升级。多学科交叉和协同创新将在后门攻击防御的研究与实践中发挥重要作用，推动深度学习系统向更安全、更可靠的方向发展。







## 结论

随着深度学习在各个领域的广泛应用，模型的安全性问题变得尤为重要。后门攻击作为一种隐蔽性高、破坏性强的威胁，能够在不显著影响模型整体性能的情况下，通过特定的触发器使模型做出攻击者预期的错误决策。本文通过对后门攻击的研究与实验，揭示了其在联邦学习环境中的潜在风险。实验结果表明，后门攻击不仅显著干扰了全局模型的主任务性能，还可能通过多轮训练持续影响全局模型的稳定性和鲁棒性。

为应对这一威胁，本文提出了一种创新性的后门攻击防御策略，旨在构建一套动态、精确且高效的防御机制。该方法围绕动态监测、恶意更新识别、自适应处理与模型聚合展开，通过多层次的防御措施最大限度地减少恶意客户端对全局模型的影响。首先，本文引入了**动态模型分布分析**方法，通过统计手段对客户端上传的模型更新分布特性进行实时分析，基于异常得分机制，动态监测并精准定位潜在的恶意客户端。这种机制打破了传统防御方法对静态攻击模式的假设，增强了面对复杂化和多样化攻击时的适应性与鲁棒性。其次，本文提出了**自适应触发器识别与裁剪**方法，通过多模态特征提取与聚类方法，识别恶意触发器对全局模型的影响，并利用自适应裁剪策略剔除恶意更新。这种方法兼具高效性和低计算开销，同时确保正常客户端的更新不受影响。最后，本文优化了**全局模型聚合与更新机制**，在剔除恶意更新后对客户端更新进行全局聚合，以平衡模型的安全性和分类性能。

与现有防御方法相比，本文提出的防御策略具有显著的创新性和优势。动态模型分布分析提高了防御方法在复杂攻击场景中的适用性，自适应触发器识别方法增强了对攻击模式的泛化能力，而全局聚合机制则在模型性能与安全性之间实现了良好的平衡。实验结果进一步验证了该方法在多种后门攻击场景下的有效性和鲁棒性，同时展示了其在保持模型主任务性能方面的优势。

未来的研究可以围绕以下方向展开：一是优化动态监测算法，进一步提升对恶意更新的检测效率；二是扩展防御策略的适用范围，涵盖更多类型的深度学习模型与任务；三是探索分布式环境下的联合防御机制，结合多方协作进一步增强防御能力。此外，针对复杂攻击场景下的低开销防御算法的研究，以及与多学科协同的防御体系构建，也将成为后续工作的重点。

综上，本文通过创新性的防御策略为联邦学习环境中的后门攻击问题提供了有效的解决方案。这些研究不仅为后门攻击防御的理论与实践提供了重要参考，也为深度学习系统的安全性提升奠定了坚实的基础。未来，深度学习技术的快速发展需要与安全防护技术相辅相成，只有构建更加可靠的防御体系，才能使人工智能技术在更广泛的应用场景中发挥其真正的潜力。













参考文献

(1) Gu, Tianyu et al. BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. (arxiv 2017)

(2) Matthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, Susmit Jha: Dual-Key Multimodal Backdoors for Visual Question Answering. CVPR 2022: 15354-15364

(3) Xingshuo Han, Yutong Wu, Qingjie Zhang, Yuan Zhou, Yuan Xu, Han Qiu, Guowen Xu, Tianwei Zhang:Backdooring Multimodal Learning. SP 2024: 3385-3403

(4)Indranil Sur, Karan Sikka, Matthew Walmer, Kaushik Koneripalli, Anirban Roy, Xiao Lin, Ajay Divakaran, Susmit Jha:TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models. ICCV 2023: 165-175

(5)Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, Wei Liu

BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP



